{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064f433e-2c9e-42cf-b2d2-c3fe8561f19d",
   "metadata": {},
   "source": [
    "<H1>Datascraping Weather Data from Web Search</H1>\n",
    "<p><STRONG>Author:</STRONG> Siraj Sabihuddin, <STRONG>Date:</STRONG> July 27, 2021</p>\n",
    "<p>The below code demonstrates the basics of data scraping datascraping. The goal of this code is very simple. Its to go into some search engines and grab some weather data from the search results for a particular region. To help with this process I'll be refering to the following tutorials as well:</p> \n",
    "\n",
    "<ol>\n",
    "  <li><a href=https://aris-pattakos.medium.com/advanced-web-scraping-concepts-to-help-you-get-unstuck-17c0203de7ab>https://aris-pattakos.medium.com/advanced-web-scraping-concepts-to-help-you-get-unstuck-17c0203de7ab</a></li>\n",
    "  <li><a href=https://www.geeksforgeeks.org/how-to-extract-weather-data-from-google-in-python/>https://www.geeksforgeeks.org/how-to-extract-weather-data-from-google-in-python/</a></li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0d4db-0149-436a-8905-11136bd30105",
   "metadata": {},
   "source": [
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f2992-5531-4b26-a05c-625d5b9e0e7f",
   "metadata": {},
   "source": [
    "The first step in this process is some imports. In this case there are some basic tools we need for web scarping. The <STRONG>requests</STRONG> library provides tools for fetching HTML files from the web. Combined with a custom library directly from git called <STRONG>request_files</STRONG> that can be used to allow loading of a local HTML file as well. You can find the requests-file repository at: https://github.com/dashea/requests-file. To install it follow the instructions at: https://medium.com/i-want-to-be-the-very-best/installing-packages-from-github-with-conda-commands-ebf10de396f4. Finally <STRONG>BeautifulSoup</STRONG> allows for parsing of HTML data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d9a2d5-72f1-4b9c-a292-0973388bd8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests                                  # For fetching HTML files\n",
    "from bs4 import BeautifulSoup                    # For Parsing HTML text data\n",
    "from requests_file import FileAdapter            # For fetching local HTML files\n",
    "import os                                        # For getting the directory path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f39925-ffe3-417e-b5f4-f77555678cfe",
   "metadata": {},
   "source": [
    "At this point we can grab the input from the user for a particular location for which we want to grab weather data. The format of this data is important. Urls need to have spaces and the like properly converted to be recognizable. So this must be done. Likewise we need to add language and locality information to the query to make sure we are searching and getting results in the right language. Finally we also need to make sure the right browser identifier is being used for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf0944f-6b3a-45ad-bfe9-111eda5051b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the City Name:  Taipei\n"
     ]
    }
   ],
   "source": [
    "# Enter the City Name\n",
    "city = input(\"Enter the City Name: \")\n",
    "search = \"Weather+in+{}\".format(city)\n",
    "\n",
    "# Construct the query URL string for google. \n",
    "lang = \"hl=en&gl=en\"\n",
    "url = f\"http://www.google.com/search?q={search}&{lang}\" \n",
    "#url = f\"file:///G:/My%20Drive/Random/test.html\"\n",
    "\n",
    "# Setup the request so that it has the proper \n",
    "# browser identified and setup the header for the request\n",
    "head = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36' }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6346b4d-e17e-48b5-8b2b-a09a983d9484",
   "metadata": {},
   "source": [
    "Now we are ready to grab the web data. Traditionally we would do this as shown below with the use of requests. This was if we were not going ahead and using a headless browser. Once the request is made using this approach we can store it and use it later for BeautifulSoup parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca1036d-13a0-442a-818d-e38cca4a4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup requests local file adapter for fetching local files\n",
    "req_ = requests.Session()\n",
    "req_.mount('file://', FileAdapter())\n",
    "\n",
    "# Send HTTP request. \n",
    "# Pull HTTP data from internet or local file\n",
    "req = req_.get(url, headers=head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42940ba9-ca26-41de-b0ec-8f27ee29bfd4",
   "metadata": {},
   "source": [
    "Once the data has been collected we use Beautiful Soup to get the contents. We can make these prettier and display them as well. We see when we display the prettified version of the HTML fetched from google in this way that there is a big difference between the Inspect element version and the verion obtained from direct request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aabd7bd1-f3de-463f-b671-740d372b0d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the HTML received\n",
    "sor = BeautifulSoup(req.content) \n",
    "\n",
    "# Clean up the html for viewing (this is for debugging \n",
    "# and figuring out the right class names etc. to extract from)\n",
    "prettyhtml = sor.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea269d2-b2a0-447d-9aee-10920771c1ed",
   "metadata": {},
   "source": [
    "We can do this comparison by saving the prettified version. To save the file we need to reconstruct the local directory base path and store it. You can find out how to do this here: https://www.makeuseof.com/how-to-get-the-current-directory-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7673f8b5-7588-41b6-8590-1465562ff7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a full path based on the operating system being used.\n",
    "current_dir = os.getcwd()\n",
    "prettyhtmlfile = r'extracted_html.html'\n",
    "full_path =os.path.join(current_dir, prettyhtmlfile)\n",
    "\n",
    "# Save the file with the constructed full path\n",
    "save_file = open(full_path, 'w', encoding=\"utf-8\")\n",
    "save_file.write(prettyhtml)\n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cb44c-ae73-4b83-80c6-b1db2efcd259",
   "metadata": {},
   "source": [
    "Now from here we can find the temperature using the id \"wob_tm\". Likewise we an inspect the HTML and find other data such as the precipitation (\"wob_pp\"), humidity (\"wob_hm\") and wind speed (\"wob_ws\"). In the case that we fail to add a header, the delivered HTML isn't the same as that as we see when the header is included. Without the header we need to look for the BNeawe class which is actually not present in the inspect element version of the HTML. Thus this second class is specific to the HTML fetched through the requests library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8556073-896f-4470-96b5-7c61a4c0b599",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temperature in Taipei is 28 C\n",
      "The chance of precipitation in Taipei is 13%\n",
      "The relative humidity in Taipei is 99%\n",
      "The wind speed in Taipei is 3 km/h\n"
     ]
    }
   ],
   "source": [
    "# Find the temperature data in Celsius\n",
    "temp = sor.find('span', attrs={'id': 'wob_tm'}).text \n",
    "# Find the precipitation chance data in %\n",
    "precip = sor.find('span', attrs={'id': 'wob_pp'}).text\n",
    "# Find the humidity in %\n",
    "humid = sor.find('span', attrs={'id': 'wob_hm'}).text\n",
    "# Find the wind speed in km/h\n",
    "wind = sor.find('span', attrs={'id': 'wob_ws'}).text\n",
    "\n",
    "# Output data\n",
    "print('The temperature in {} is {} C'.format(city, temp))\n",
    "print('The chance of precipitation in {} is {}'.format(city,precip))\n",
    "print('The relative humidity in {} is {}'.format(city,humid))\n",
    "print('The wind speed in {} is {}'.format(city,wind))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645d4f2-3f77-46e4-b369-6f197dd23de3",
   "metadata": {},
   "source": [
    "There were some problems making get requests for a url. When the request is made without specifying the headers, it grabs the HTML delivered before all the javascript dynamic content is loaded. This makes it look very different from the Inspect Element (Browser function) version of the HTML sometimes which has very different HTML elements in the case of google than that delivered by <STRONG>requests.get()</STRONG>. To alleviate this problem at the cost of slowness, I'm using the <STRONG>selenium</STRONG> library to create a headless browser and control the browser directly to extract the final HTML. More details at: https://selenium-python.readthedocs.io/getting-started.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d384ec2c-a7ab-42b5-93b4-2d740d254abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver                         # Using a headless web browser \n",
    "from selenium.webdriver.common.keys import Keys        # Provides keyboard input into headless browser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e1e6f-5d94-456a-ab6e-eb4d0e768df0",
   "metadata": {},
   "source": [
    "To use selenium, say with chromium, I need to also have a driver for chrome installed and placed in the appropriate path. The same is needed for firefox, safari, edge, etc. The URL at: https://selenium-python.readthedocs.io/installation.html explains how to install selenium and get it running quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c733f58-4676-4a94-ad90-57c652439f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromeOptions = webdriver.ChromeOptions() \n",
    "chromeOptions.add_argument(\"--remote-debugging-port=9222\")\n",
    "driver = webdriver.Chrome(options=chromeOptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed0527a-5064-40f6-ac9b-d76084097099",
   "metadata": {},
   "source": [
    "Alternatively, we can use the browser approach with selenium. Here we grab the url and do an element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce965603-65a3-4437-9037-c68ef44d64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcacd2-361b-45c2-854a-6a41bdf31508",
   "metadata": {},
   "source": [
    "As with the previous direct attempt at data-scraping, we can extract the temperature, precipitation, humidity and wind speed data through selenium. The difference lies in making a call directly to chrome libraries and extracting the data through <STRONG>find_element_by_id</STRONG>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1034c5b1-ba34-4aa3-8d91-542f77485360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temperature in Taipei is 28 C\n",
      "The chance of precipitation in Taipei is 13%\n",
      "The relative humidity in Taipei is 99%\n",
      "The wind speed in Taipei is 3 km/h\n"
     ]
    }
   ],
   "source": [
    "# Find the temperature data in Celsius\n",
    "temp = driver.find_element_by_id(\"wob_tm\").text\n",
    "precip = driver.find_element_by_id(\"wob_pp\").text\n",
    "humid = driver.find_element_by_id(\"wob_hm\").text\n",
    "wind = driver.find_element_by_id(\"wob_ws\").text\n",
    "\n",
    "# Output temperature\n",
    "print('The temperature in {} is {} C'.format(city, temp))\n",
    "print('The chance of precipitation in {} is {}'.format(city,precip))\n",
    "print('The relative humidity in {} is {}'.format(city,humid))\n",
    "print('The wind speed in {} is {}'.format(city,wind))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c04c68-de13-49cd-a3e2-f8d9c515d348",
   "metadata": {},
   "source": [
    "Once the data has been captured we can close the selenium browser and end the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "172e5bd9-fb2e-40db-86a1-a0aedffedb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
