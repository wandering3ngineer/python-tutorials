<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>llm API documentation</title>
<meta name="description" content="Contains code for starting and executing various LLMs. On the front
side this code will wait for RESTful requests and then restart
the relevant …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>llm</code></h1>
</header>
<section id="section-intro">
<p>Contains code for starting and executing various LLMs. On the front
side this code will wait for RESTful requests and then restart
the relevant private LLMs or stop them and use an external LLM such
as openai's gpt-4. To support queries of these individual LLM processes
the code relays requests coming from external source as is if the
appropriate relay endpoint is provided. It relies on the fact that
all models use openai's RESTful api format.
</p>
<p>Author:
Siraj Sabihuddin</p>
<p>Date:
June 28, 2024</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="llm.configLoad"><code class="name flex">
<span>def <span class="ident">configLoad</span></span>(<span>config_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Grab the stored config data in the config file</p>
<h2 id="args">Args</h2>
<p>config_file : str =
The path to the JSON config file
Returns: <br>
config : dict =
Json dictionary of values from json file</p>
<pre><code>model_index : int = 
    The index of the active model
</code></pre></div>
</dd>
<dt id="llm.configStore"><code class="name flex">
<span>def <span class="ident">configStore</span></span>(<span>config_file, config)</span>
</code></dt>
<dd>
<div class="desc"><p>Store the updated config data in the config file</p>
<h2 id="args">Args</h2>
<p>config_file : str =
The path to the JSON config file</p>
<p>config : dict =
The updated dictionary of configuration
data</p></div>
</dd>
<dt id="llm.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the main function it executes and starts the uvicorn server
for llama-cpp-python and configures the server to run at a specific
IP:Port and using a specific private model</p></div>
</dd>
<dt id="llm.model"><code class="name flex">
<span>async def <span class="ident">model</span></span>(<span>model)</span>
</code></dt>
<dd>
<div class="desc"><p>Changes the model to the indicated model value if it exists. This
requires that the existing model is stopped and a new model is
restarted on the server</p>
<p>Route:
@app.put("/model/{model}")</p>
<h2 id="args">Args</h2>
<p>model : str =
The model name to change to. Note that valid model
names are located in the config file</p></div>
</dd>
<dt id="llm.relay"><code class="name flex">
<span>async def <span class="ident">relay</span></span>(<span>request: starlette.requests.Request, path: str)</span>
</code></dt>
<dd>
<div class="desc"><p>This function acts as a relay. Basically any request sent to the endpoint
will be relayed on to either a public or private LLM. </p>
<h2 id="route">Route</h2>
<p>@app.api_route("/relay/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH"])</p>
<h2 id="args">Args</h2>
<p>path : str =
The path after the host:port name</p>
<p>request : Request =
The fastAPI Request object</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>response </code></dt>
<dd>Response =
The FastAPI response object</dd>
</dl></div>
</dd>
<dt id="llm.run_api"><code class="name flex">
<span>def <span class="ident">run_api</span></span>(<span>host, port, pid=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This function should be called to start the server for the API microservice
for the LLM. The API service allows users to change and start a different
private model or to call an external model such as that of openai </p>
<h2 id="args">Args</h2>
<p>host : str =
The host ip address passed in as a string</p>
<p>port : str =
The host port passed in as a string
</p>
<p>pid : int =
The pid to existing process
if it exists</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pid </code></dt>
<dd>int =
Process pid for allowing termination from outside
the run command</dd>
</dl></div>
</dd>
<dt id="llm.run_llm"><code class="name flex">
<span>def <span class="ident">run_llm</span></span>(<span>model, host, port, pid=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This function should be called to start the server app microservice
for the LLM. The LLM then waits for API requests from another microservice.
The API format is the OpenAI API format </p>
<h2 id="args">Args</h2>
<p>model : str =
The path to the LLM model of interest to us.
This should be a .gguf model file. </p>
<p>host : str =
The host ip address passed in as a string</p>
<p>port : str =
The host port passed in as a string
</p>
<p>pid : int =
The pid to existing process
if it exists</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pid </code></dt>
<dd>int =
Process pid for allowing termination from outside
the run command</dd>
</dl></div>
</dd>
<dt id="llm.terminate"><code class="name flex">
<span>def <span class="ident">terminate</span></span>(<span>config, model=True, api=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Terminates the processes in the config data and updates
the configuration to reflect the new pid values of null</p>
<h2 id="args">Args</h2>
<p>config : dict =
The path to the JSON config file</p></div>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="llm.configLoad" href="#llm.configLoad">configLoad</a></code></li>
<li><code><a title="llm.configStore" href="#llm.configStore">configStore</a></code></li>
<li><code><a title="llm.main" href="#llm.main">main</a></code></li>
<li><code><a title="llm.model" href="#llm.model">model</a></code></li>
<li><code><a title="llm.relay" href="#llm.relay">relay</a></code></li>
<li><code><a title="llm.run_api" href="#llm.run_api">run_api</a></code></li>
<li><code><a title="llm.run_llm" href="#llm.run_llm">run_llm</a></code></li>
<li><code><a title="llm.terminate" href="#llm.terminate">terminate</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
